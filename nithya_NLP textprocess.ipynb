{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "import re\n",
    "import xml\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from ast import literal_eval\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nravi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nravi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nravi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLP_Nithya():\n",
    "    def __init__(self, remove_stopwords=False, lower_case = False, white_space = True,\n",
    "             remove_numbers= False, remove_html_tags=False, lemmatize_method='wordnet',add_stopwords = False,\n",
    "             remove_punctuations= False,remove_escapesequence = False, lemmatize=False):\n",
    "        if (type(remove_stopwords) != bool or\n",
    "            type(lower_case) != bool or\n",
    "            type(white_space) != bool or\n",
    "            type(remove_numbers) != bool or\n",
    "            type(remove_html_tags) != bool or\n",
    "            type(remove_punctuations) != bool or\n",
    "            type(lemmatize) != bool):\n",
    "            raise Exception(\"Error - expecting a boolean parameter\")\n",
    "        self.doc = None\n",
    "        self.lemmatizer = None\n",
    "        self.white_space = white_space\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.lower_case = lower_case\n",
    "        self.remove_numbers = remove_numbers\n",
    "        self.add_stopwords = add_stopwords\n",
    "        self.remove_html_tags = remove_html_tags\n",
    "        self.remove_escapesequence = remove_escapesequence\n",
    "        self.remove_punctuations = remove_punctuations\n",
    "        self.lemmatize = lemmatize\n",
    "        self.lemmatize_method = lemmatize_method\n",
    "        if self.lemmatize_method == 'wordnet':\n",
    "            self.lemmatizer = WordNetLemmatizer()\n",
    "        if self.lemmatize_method == 'snowball':\n",
    "            self.lemmatizer = SnowballStemmer('english')\n",
    "        \n",
    "    def punctuation(self):   \n",
    "        #import string #When I rerun it is not recognizing string.punctuation\n",
    "        a  = []\n",
    "        tk = WordPunctTokenizer()\n",
    "        for d in self.doc:\n",
    "            punct = tk.tokenize(d)\n",
    "            pun = [''.join(c for c in s if c not in string.punctuation) for s in self.doc]\n",
    "            punctuation = [s for s in pun if s]\n",
    "            self.doc = ''.join(punctuation)\n",
    "        return self.doc\n",
    "\n",
    "    def lemmatizer_word(self):\n",
    "        tokens = str(self.doc).split()\n",
    "        cleaned_tokens = None\n",
    "        if self.lemmatize_method == 'wordnet':\n",
    "            cleaned_tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "        else:\n",
    "            cleaned_tokens = [self.lemmatizer.stem(token) for token in tokens]\n",
    "       \n",
    "        self.doc = ' '.join(cleaned_tokens)\n",
    "        return self.doc\n",
    "\n",
    "    def remove_stop_words(self):\n",
    "        stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "        results = []\n",
    "        total_results = []\n",
    "        for d in [self.doc]:\n",
    "            word_tokens = word_tokenize(d) \n",
    "            filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "            filtered_sentence = [] \n",
    "            a = []\n",
    "            for w in word_tokens: \n",
    "                if w not in stop_words: \n",
    "                    filtered_sentence.append(w) \n",
    "            separator = ' '\n",
    "            results = separator.join(filtered_sentence)\n",
    "            a.append(results)\n",
    "            total_results.append(separator.join(a))\n",
    "        self.doc = total_results\n",
    "        return self.doc\n",
    "    def add_stop_words(self):\n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "        for i in [self.doc]:\n",
    "            if(i not in stopwords):\n",
    "                stopwords.append(i)\n",
    "            else:\n",
    "                print(\"'\"+i+\"'\",\"is already in the list so it was not added\")\n",
    "        print(stopwords)\n",
    "        return self.doc\n",
    "        \n",
    "    def lower_cases(self):\n",
    "        total_results = [x.lower() for x in self.doc]\n",
    "        s = ''.join(total_results)\n",
    "        self.doc = s\n",
    "        return self.doc\n",
    "    def remove_html_tags_words(self):\n",
    "        listhtml = []\n",
    "        for html in self.doc:\n",
    "            soup = BeautifulSoup(html, 'lxml')\n",
    "            listhtml.append(soup.text)\n",
    "        self.doc = listhtml\n",
    "        return self.doc\n",
    "    def number_remover(self):\n",
    "        res_num = []\n",
    "        for str in self.doc:\n",
    "            result = ''.join([i for i in str if not i.isdigit()])\n",
    "            res_num.append(result)\n",
    "        s = ''.join(res_num)\n",
    "        self.doc = s\n",
    "        return self.doc\n",
    "    def white_space_remover(self):\n",
    "        res = []\n",
    "        for d in self.doc:\n",
    "            result = \" \".join(d.split())\n",
    "            res.append(result)\n",
    "            #print(res)\n",
    "        self.doc = res\n",
    "        return self.doc\n",
    "    \n",
    "    def escape_sequence(self):\n",
    "        Tmp = []\n",
    "        for l in self.doc:\n",
    "            ansi_escape = re.compile(r'\\x1b[^m]*m')\n",
    "            g = ansi_escape.sub(\"\", str(l))\n",
    "            Tmp.append(g.strip())\n",
    "        self.doc = Tmp\n",
    "        return self.doc\n",
    "    def process(self, doc):\n",
    "        self.doc = [doc]\n",
    "        if self.white_space is True:\n",
    "            self.white_space_remover()\n",
    "        if self.lower_case is True:\n",
    "            self.lower_cases()\n",
    "        if self.remove_html_tags is True:\n",
    "            self.remove_html_tags_words()\n",
    "        if self.remove_numbers is True:\n",
    "            self.number_remover()\n",
    "        if self.remove_stopwords is True:\n",
    "            self.remove_stop_words()\n",
    "        if self.add_stopwords is True:\n",
    "            self.add_stop_words()\n",
    "        if self.remove_escapesequence is True:\n",
    "            self.escape_sequence()\n",
    "        if self.remove_punctuations is True:\n",
    "            self.punctuation() \n",
    "        if self.lemmatize is True:\n",
    "            self.lemmatizer_word()\n",
    "        return self.doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = NLP_Nithya(remove_stopwords = True, remove_numbers = True, remove_punctuations = True, white_space = True, lemmatize = True, remove_html_tags = True, remove_escapesequence = True)\n",
    "h = [\"This is a      sample#@$ sentence, corpora  \\n\\t   showing off     the stop### words filtration.\", \"Everything&&*3 is perfect.\", \"<p>My bases have been in the best way305984</p>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This sample sentence corpus showing stop word filtration',\n",
       " 'Everything perfect',\n",
       " 'My base best way']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = map(obj.process,h)\n",
    "new_list = list(h)\n",
    "new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
